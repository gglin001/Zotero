
Cornell University
We gratefully acknowledge support from
the Simons Foundation and member institutions.
arXiv.org > cs > arXiv:1809.09293
( Help | Advanced search )
Full-text links:
Download:

    PDF
    Other formats

( license )
Current browse context:
cs.HC
< prev  |  next >
new  | recent  | 1809
Change to browse by:
cs
cs.AI
cs.CV
References & Citations

    NASA ADS

DBLP - CS Bibliography
listing | bibtex
Vaneet Aggarwal
Hamed Asadi
Mayank Gupta
Jae Joong Lee
Denny Yu
Google Scholar
Bookmark
( what is this? )
CiteULike logo BibSonomy logo Mendeley logo Reddit logo ScienceWISE logo
Computer Science > Human-Computer Interaction
Title: Covfefe: A Computer Vision Approach For Estimating Force Exertion
Authors: Vaneet Aggarwal , Hamed Asadi , Mayank Gupta , Jae Joong Lee , Denny Yu
(Submitted on 25 Sep 2018)

    Abstract: Cumulative exposure to repetitive and forceful activities may lead to musculoskeletal injuries which not only reduce workers' efficiency and productivity, but also affect their quality of life. Thus, widely accessible techniques for reliable detection of unsafe muscle force exertion levels for human activity is necessary for their well-being. However, measurement of force exertion levels is challenging and the existing techniques pose a great challenge as they are either intrusive, interfere with human-machine interface, and/or subjective in the nature, thus are not scalable for all workers. In this work, we use face videos and the photoplethysmography (PPG) signals to classify force exertion levels of 0\%, 50\%, and 100\% (representing rest, moderate effort, and high effort), thus providing a non-intrusive and scalable approach. Efficient feature extraction approaches have been investigated, including standard deviation of the movement of different landmarks of the face, distances between peaks and troughs in the PPG signals. We note that the PPG signals can be obtained from the face videos, thus giving an efficient classification algorithm for the force exertion levels using face videos. Based on the data collected from 20 subjects, features extracted from the face videos give 90\% accuracy in classification among the 100\% and the combination of 0\% and 50\% datasets. Further combining the PPG signals provide 81.7\% accuracy. The approach is also shown to be robust to the correctly identify force level when the person is talking, even though such datasets are not included in the training. 

Comments: 	12 pages
Subjects: 	Human-Computer Interaction (cs.HC) ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)
Cite as: 	arXiv:1809.09293 [cs.HC]
  	(or arXiv:1809.09293v1 [cs.HC] for this version)
Try the Bibliographic Explorer
(can be disabled at any time)
Enable Don't show again
Bibliographic data
[ Enable Bibex ( What is Bibex? )]
Submission history
From: Vaneet Aggarwal [ view email ]
[v1] Tue, 25 Sep 2018 02:45:19 UTC (1,746 KB)
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) Browse v0.1 released 2018-10-22    Feedback?

    About arXiv
    Leadership Team

    Contact Us
    Follow us on Twitter

    Help
    Privacy Policy

    Blog
    Subscribe

arXivÂ® is a registered trademark of Cornell University.

If you have a disability and are having trouble accessing information on this website or need materials in an alternate format, contact web-accessibility@cornell.edu for assistance.
